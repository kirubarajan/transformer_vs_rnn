# Transformers vs Recurrent Neural Networks
> An analysis on the trade-offs between NLP's standard neural models.

## Abstract
Transformer models have recently become the state of the art for a variety of natural language processing tasks (e.g. summarization, dialogue, translation). They offer computational benefits over standard recurrent and feed-forward neural network architectures, pertaining to parallelization and parameter size. In this paper, we analyze the performance gains of Transformer and LSTM models as their size increases, in an effort to determine when researchers should choose Transformer architectures over recurrent neural networks. 
